{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7d63c36",
   "metadata": {},
   "source": [
    "### Sequence-to-Point architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9e18be9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\cmcru\\anaconda3\\envs\\nilmtk-env\\lib\\site-packages (2.12.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.12.0 in c:\\users\\cmcru\\anaconda3\\envs\\nilmtk-env\\lib\\site-packages (from tensorflow) (2.12.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in c:\\users\\cmcru\\anaconda3\\envs\\nilmtk-env\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\cmcru\\anaconda3\\envs\\nilmtk-env\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (4.5.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\cmcru\\anaconda3\\envs\\nilmtk-env\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\cmcru\\anaconda3\\envs\\nilmtk-env\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (59.8.0)\n",
      "Requirement already satisfied: numpy<1.24,>=1.22 in c:\\users\\cmcru\\anaconda3\\envs\\nilmtk-env\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.23.5)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\cmcru\\anaconda3\\envs\\nilmtk-env\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (3.8.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\cmcru\\anaconda3\\envs\\nilmtk-env\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\cmcru\\anaconda3\\envs\\nilmtk-env\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\cmcru\\anaconda3\\envs\\nilmtk-env\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (16.0.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\cmcru\\anaconda3\\envs\\nilmtk-env\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (23.3.3)\n",
      "Requirement already satisfied: jax>=0.3.15 in c:\\users\\cmcru\\anaconda3\\envs\\nilmtk-env\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in c:\\users\\cmcru\\anaconda3\\envs\\nilmtk-env\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (2.12.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\cmcru\\anaconda3\\envs\\nilmtk-env\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.54.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\cmcru\\anaconda3\\envs\\nilmtk-env\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.4.0)\n",
      "Requirement already satisfied: tensorboard<2.13,>=2.12 in c:\\users\\cmcru\\anaconda3\\envs\\nilmtk-env\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (2.12.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\cmcru\\anaconda3\\envs\\nilmtk-env\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (23.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\cmcru\\anaconda3\\envs\\nilmtk-env\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (4.22.3)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\cmcru\\anaconda3\\envs\\nilmtk-env\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\cmcru\\anaconda3\\envs\\nilmtk-env\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\cmcru\\anaconda3\\envs\\nilmtk-env\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\cmcru\\anaconda3\\envs\\nilmtk-env\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: keras<2.13,>=2.12.0 in c:\\users\\cmcru\\anaconda3\\envs\\nilmtk-env\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (2.12.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\cmcru\\anaconda3\\envs\\nilmtk-env\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.12.0->tensorflow) (0.40.0)\n",
      "Requirement already satisfied: scipy>=1.7 in c:\\users\\cmcru\\anaconda3\\envs\\nilmtk-env\\lib\\site-packages (from jax>=0.3.15->tensorflow-intel==2.12.0->tensorflow) (1.9.1)\n",
      "Requirement already satisfied: ml-dtypes>=0.0.3 in c:\\users\\cmcru\\anaconda3\\envs\\nilmtk-env\\lib\\site-packages (from jax>=0.3.15->tensorflow-intel==2.12.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\cmcru\\anaconda3\\envs\\nilmtk-env\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.17.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\cmcru\\anaconda3\\envs\\nilmtk-env\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.28.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\cmcru\\anaconda3\\envs\\nilmtk-env\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (3.4.3)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in c:\\users\\cmcru\\anaconda3\\envs\\nilmtk-env\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\cmcru\\anaconda3\\envs\\nilmtk-env\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\cmcru\\anaconda3\\envs\\nilmtk-env\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (0.7.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\cmcru\\anaconda3\\envs\\nilmtk-env\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\cmcru\\anaconda3\\envs\\nilmtk-env\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (0.3.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\cmcru\\anaconda3\\envs\\nilmtk-env\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (5.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\cmcru\\anaconda3\\envs\\nilmtk-env\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\cmcru\\anaconda3\\envs\\nilmtk-env\\lib\\site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\cmcru\\anaconda3\\envs\\nilmtk-env\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (6.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\cmcru\\anaconda3\\envs\\nilmtk-env\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\cmcru\\anaconda3\\envs\\nilmtk-env\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\cmcru\\anaconda3\\envs\\nilmtk-env\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (1.26.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\cmcru\\anaconda3\\envs\\nilmtk-env\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\cmcru\\anaconda3\\envs\\nilmtk-env\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.1.2)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\cmcru\\anaconda3\\envs\\nilmtk-env\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (3.15.0)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in c:\\users\\cmcru\\anaconda3\\envs\\nilmtk-env\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\cmcru\\anaconda3\\envs\\nilmtk-env\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (3.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b58a8c1",
   "metadata": {},
   "source": [
    "Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07eee762",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict  # a dictionary subclass that remembers the order that keys were first inserted\n",
    "import numpy as np  \n",
    "import pandas as pd  \n",
    "from nilmtk.disaggregate import Disaggregator  # a library for non-intrusive load monitoring\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint  # a library for saving the model weights during training\n",
    "from tensorflow.keras.layers import Conv1D, Dense, Dropout, Reshape, Flatten  # layers for building neural network models\n",
    "from tensorflow.keras.models import Sequential  # a class for building neural network models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9734bd4f",
   "metadata": {},
   "source": [
    "seq2point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "daa47b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define custom exception classes\n",
    "class SequenceLengthError(Exception):\n",
    "    pass\n",
    "\n",
    "class ApplianceNotFoundError(Exception):\n",
    "    pass\n",
    "\n",
    "# define the Seq2Point class which is a subclass of Disaggregator\n",
    "class Seq2Point(Disaggregator):\n",
    "\n",
    "    def __init__(self, params):\n",
    "        \"\"\"\n",
    "        Initialize the Seq2Point model with the specified parameters.\n",
    "        \"\"\"\n",
    "    #initialize the s2p model with the specified parameters - these are default paremeters.\n",
    "\n",
    "        # initialize the model name to \"Seq2Point\"\n",
    "        self.MODEL_NAME = \"Seq2Point\"\n",
    "        # create an ordered dictionary to store the models\n",
    "        self.models = OrderedDict()\n",
    "        # create a file prefix for the model weights\n",
    "        self.file_prefix = \"{}-temp-weights\".format(self.MODEL_NAME.lower())\n",
    "        # set the chunk-wise training parameter to False by default\n",
    "        self.chunk_wise_training = params.get('chunk_wise_training',False)\n",
    "        # set the sequence length parameter to 99 by default\n",
    "        self.sequence_length = params.get('sequence_length',99)\n",
    "        # set the number of epochs parameter to 10 by default\n",
    "        self.n_epochs = params.get('n_epochs', 10 )\n",
    "        # set the batch size parameter to 512 by default\n",
    "        self.batch_size = params.get('batch_size',512)\n",
    "        # set the appliance parameters to an empty dictionary by default\n",
    "        self.appliance_params = params.get('appliance_params',{})\n",
    "        # set the mains mean parameter to 1800 by default\n",
    "        self.mains_mean = params.get('mains_mean',1800)\n",
    "        # set the mains standard deviation parameter to 600 by default\n",
    "        self.mains_std = params.get('mains_std',600)\n",
    "        # Check if the sequence length is odd\n",
    "        if self.sequence_length%2==0: # Ensuring that sequence length is odd\n",
    "            # If it's even, raise an exception\n",
    "            print (\"Sequence length should be odd!\")\n",
    "            raise (SequenceLengthError)\n",
    "\n",
    "#train the s2p model using the given training data\n",
    "\n",
    "    def partial_fit(self, train_main, train_appliances, do_preprocessing=True, current_epoch=0, **load_kwargs):\n",
    "        \"\"\"\n",
    "        Train the Seq2Point model using the given training data.\n",
    "        \"\"\"\n",
    "        # If no appliance wise parameters are provided, then copmute them using the first chunk\n",
    "        if len(self.appliance_params) == 0:\n",
    "            self.set_appliance_params(train_appliances)\n",
    "\n",
    "        print(\"...............Seq2Point partial_fit running...............\")\n",
    "        \n",
    "        # Do the pre-processing, such as  windowing and normalizing\n",
    "        if do_preprocessing:\n",
    "            train_main, train_appliances = self.call_preprocessing(\n",
    "                train_main, train_appliances, 'train')\n",
    "       \n",
    "    # Reshape the training data for input to the neural network\n",
    "        train_main = pd.concat(train_main, axis=0)\n",
    "        train_main = train_main.values.reshape((-1, self.sequence_length, 1))\n",
    "        new_train_appliances = []\n",
    "        for app_name, app_df in train_appliances:\n",
    "            app_df = pd.concat(app_df, axis=0)\n",
    "            app_df_values = app_df.values.reshape((-1, 1))\n",
    "            new_train_appliances.append((app_name, app_df_values))\n",
    "        train_appliances = new_train_appliances\n",
    "\n",
    "        for appliance_name, power in train_appliances:\n",
    "            # Check if the appliance was already trained. If not then create a new model for it\n",
    "            if appliance_name not in self.models:\n",
    "                print(\"First model training for\", appliance_name)\n",
    "                self.models[appliance_name] = self.return_network()\n",
    "            # Retrain the particular appliance\n",
    "            else:\n",
    "                print(\"Started Retraining model for\", appliance_name)\n",
    "\n",
    "            model = self.models[appliance_name]\n",
    "            if train_main.size > 0:\n",
    "                # Sometimes chunks can be empty after dropping NANS\n",
    "                if len(train_main) > 10:\n",
    "                    # Do validation when you have sufficient samples\n",
    "                    filepath = self.file_prefix + \"-{}-epoch{}.h5\".format(\n",
    "                            \"_\".join(appliance_name.split()),\n",
    "                            current_epoch,\n",
    "                    )\n",
    "                    checkpoint = ModelCheckpoint(filepath,monitor='val_loss',verbose=1,save_best_only=True,mode='min')\n",
    "                    model.fit(\n",
    "                            train_main, power,\n",
    "                            validation_split=0.15,\n",
    "                            epochs=self.n_epochs,\n",
    "                            batch_size=self.batch_size,\n",
    "                            callbacks=[checkpoint],\n",
    "                    )\n",
    "                    model.load_weights(filepath)\n",
    "\n",
    "                    \n",
    "    def disaggregate_chunk(self,test_main_list,model=None,do_preprocessing=True):\n",
    "        \"\"\"\n",
    "        Test the model with given test data\n",
    "        \"\"\"\n",
    "        #Explanation:\n",
    "# This method is used to disaggregate a chunk of test data using the trained Seq2Point model. If a model is already\n",
    "# provided, it uses that model. Otherwise, it uses the model stored in the Seq2Point object. Before disaggregating,\n",
    "# the method preprocesses the test mains such as windowing and normalizing. Then, it predicts the power consumption\n",
    "# of each appliance using the model and stores the results in a dictionary. Finally, the dictionary is converted to a\n",
    "# pandas DataFrame and the results are returned.\n",
    "\n",
    "        if model is not None:\n",
    "            self.models = model\n",
    "\n",
    "        # Preprocess the test mains such as windowing and normalizing\n",
    "\n",
    "        if do_preprocessing:\n",
    "            test_main_list = self.call_preprocessing(test_main_list, submeters_lst=None, method='test')\n",
    "\n",
    "        test_predictions = []\n",
    "        for test_main in test_main_list:\n",
    "            test_main = test_main.values\n",
    "            test_main = test_main.reshape((-1, self.sequence_length, 1))\n",
    "            disggregation_dict = {}\n",
    "            for appliance in self.models:\n",
    "                prediction = self.models[appliance].predict(test_main,batch_size=self.batch_size)\n",
    "                prediction = self.appliance_params[appliance]['mean'] + prediction * self.appliance_params[appliance]['std']\n",
    "                valid_predictions = prediction.flatten()\n",
    "                valid_predictions = np.where(valid_predictions > 0, valid_predictions, 0)\n",
    "                df = pd.Series(valid_predictions)\n",
    "                disggregation_dict[appliance] = df\n",
    "            results = pd.DataFrame(disggregation_dict, dtype='float32')\n",
    "            test_predictions.append(results)\n",
    "        return test_predictions\n",
    "\n",
    "    def return_network(self):\n",
    "        \"\"\"\n",
    "        Define the neural network model\n",
    "        \"\"\"\n",
    "        \n",
    "        # Model architecture\n",
    "        \n",
    "        #create a sequential model\n",
    "        model = Sequential() \n",
    "        # add a 1D convolutional layer with 30 filters, kernel size of 10, ReLU activation, and input shape of (self.sequence_length, 1)\n",
    "        model.add(Conv1D(30,10,activation=\"relu\",input_shape=(self.sequence_length,1),strides=1))\n",
    "        # add another 1D convolutional layer with 30 filters, kernel size of 8, and ReLU activation\n",
    "        model.add(Conv1D(30, 8, activation='relu', strides=1))\n",
    "        # add another 1D convolutional layer with 40 filters, kernel size of 6, and ReLU activation\n",
    "        model.add(Conv1D(40, 6, activation='relu', strides=1))\n",
    "         # add another 1D convolutional layer with 50 filters, kernel size of 5, and ReLU activation\n",
    "        model.add(Conv1D(50, 5, activation='relu', strides=1))\n",
    "        # add a dropout layer with a rate of 0.2\n",
    "        model.add(Dropout(.2))\n",
    "        # add another 1D convolutional layer with 50 filters, kernel size of 5, and ReLU activation\n",
    "        model.add(Conv1D(50, 5, activation='relu', strides=1))\n",
    "        # add another dropout layer with a rate of 0.2\n",
    "        model.add(Dropout(.2))\n",
    "        # flatten the output of the previous layers\n",
    "        model.add(Flatten())\n",
    "        # add a dense layer with 1024 neurons and ReLU activation\n",
    "        model.add(Dense(1024, activation='relu'))\n",
    "        # add another dropout layer with a rate of 0.2\n",
    "        model.add(Dropout(.2))\n",
    "         # add a dense layer with 1 neuron (for output) and no activation function\n",
    "        model.add(Dense(1))\n",
    "        # compile the model with mean squared error loss and Adam optimizer\n",
    "        model.compile(loss='mse', optimizer='adam')  # ,metrics=[self.mse])\n",
    "        # return the compiled model\n",
    "        return model\n",
    "\n",
    "    def call_preprocessing(self, mains_lst, submeters_lst, method):\n",
    "        \"\"\"\n",
    "        Preprocess the given data\n",
    "        \"\"\"\n",
    "        if method == 'train':\n",
    "            # Preprocessing for the train data\n",
    "            \n",
    "            mains_df_list = []\n",
    "            # loop over the mains signals and preprocess them\n",
    "            for mains in mains_lst:\n",
    "                # Flattening the 2D dataframe into a 1D array\n",
    "                new_mains = mains.values.flatten()\n",
    "                n = self.sequence_length\n",
    "                units_to_pad = n // 2\n",
    "                # Padding zeros to ensure the first and last data points are used as a part of the window\n",
    "                new_mains = np.pad(new_mains,(units_to_pad,units_to_pad),'constant',constant_values=(0,0))\n",
    "                # Splitting the data into overlapping windows of length sequence_length\n",
    "                new_mains = np.array([new_mains[i:i + n] for i in range(len(new_mains) - n + 1)])\n",
    "                # Normalize the data\n",
    "                new_mains = (new_mains - self.mains_mean) / self.mains_std\n",
    "                mains_df_list.append(pd.DataFrame(new_mains))\n",
    "\n",
    "            appliance_list = []\n",
    "            for app_index, (app_name, app_df_list) in enumerate(submeters_lst):\n",
    "                if app_name in self.appliance_params:\n",
    "                    app_mean = self.appliance_params[app_name]['mean']\n",
    "                    app_std = self.appliance_params[app_name]['std']\n",
    "                else:\n",
    "                    # Raise an exception if the appliance is not found\n",
    "                    print (\"Parameters for \", app_name ,\" were not found!\")\n",
    "                    raise ApplianceNotFoundError()\n",
    "\n",
    "                processed_appliance_dfs = []\n",
    "\n",
    "                for app_df in app_df_list:\n",
    "                    # Normalize the data\n",
    "                    new_app_readings = app_df.values.reshape((-1, 1))\n",
    "                    # This is for choosing windows\n",
    "                    new_app_readings = (new_app_readings - app_mean) / app_std  \n",
    "                    # Return as a list of dataframe\n",
    "                    processed_appliance_dfs.append(pd.DataFrame(new_app_readings))\n",
    "                appliance_list.append((app_name, processed_appliance_dfs))\n",
    "            return mains_df_list, appliance_list\n",
    "\n",
    "        else:\n",
    "            # Preprocessing for the test data\n",
    "            mains_df_list = []\n",
    "\n",
    "            for mains in mains_lst:\n",
    "                # Flattening the 2D dataframe into a 1D array\n",
    "                new_mains = mains.values.flatten()\n",
    "                n = self.sequence_length\n",
    "                units_to_pad = n // 2\n",
    "                # Padding zeros to ensure the first and last data points are used as a part of the window\n",
    "                new_mains = np.pad(new_mains,(units_to_pad,units_to_pad),'constant',constant_values=(0,0))\n",
    "                # Splitting the data into overlapping windows of length sequence_length\n",
    "                new_mains = np.array([new_mains[i:i + n] for i in range(len(new_mains) - n + 1)])\n",
    "                # Normalize the data\n",
    "                new_mains = (new_mains - self.mains_mean) / self.mains_std\n",
    "                mains_df_list.append(pd.DataFrame(new_mains))\n",
    "            return mains_df_list\n",
    "\n",
    "    def set_appliance_params(self,train_appliances):\n",
    "        \"\"\"\n",
    "        Set the appliance parameters such as mean and std deviation\n",
    "        \"\"\"\n",
    "        # Loop through each appliance and its dataframes in the training data\n",
    "        for (app_name,df_list) in train_appliances:\n",
    "            # Concatenate all dataframes into one numpy array\n",
    "            l = np.array(pd.concat(df_list,axis=0))\n",
    "             # Calculate the mean and standard deviation of the concatenated array\n",
    "            app_mean = np.mean(l)\n",
    "            app_std = np.std(l)\n",
    "            #If the standard deviation is too small, set it to 100 (arbitrary value)\n",
    "            if app_std<1:\n",
    "                app_std = 100\n",
    "            # Add the mean and std deviation to the appliance parameters dictionary\n",
    "            self.appliance_params.update({app_name:{'mean':app_mean,'std':app_std}})\n",
    "        # Print the appliance parameters dictionary\n",
    "        print (self.appliance_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3574864c",
   "metadata": {},
   "source": [
    "### Seq2Point model:\n",
    "\n",
    "1. An instance of the Seq2Point class is created with the specified parameters.\n",
    "2. The partial_fit method is called with the training data to train the Seq2Point model.\n",
    "3. If no appliance wise parameters are provided, the set_appliance_params method is called to compute them using the first chunk of the training data.\n",
    "4. The call_preprocessing method is called to preprocess the training data.\n",
    "5. The training data is reshaped for input to the neural network and new train appliances are created with preprocessed values.\n",
    "6. The return_network method is called to define the neural network model.\n",
    "7. The neural network model is compiled with mean squared error loss and Adam optimizer.\n",
    "8. The fit method is called on the model to train it on the training data for each appliance.\n",
    "9. The trained model weights are saved in the form of a file.\n",
    "10. The disaggregate_chunk method is called with the test data to disaggregate it using the trained Seq2Point model.\n",
    "11. The call_preprocessing method is called to preprocess the test data.\n",
    "12. The test data is disaggregated using the trained model and the results are stored in a dictionary.\n",
    "13. The dictionary is converted to a pandas DataFrame and the results are returned.\n",
    "\n",
    "Some steps may be repeated depending on the number of chunks of training and test data provided."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655dddb1",
   "metadata": {},
   "source": [
    "### Train & Test split\n",
    "\n",
    "The code provided for Seq2Point does not include a function to split the data into training and testing sets. Instead, it assumes that you have already split your data into training and testing sets and provided those sets as inputs to the partial_fit() and disaggregate_chunk() methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf5b82d",
   "metadata": {},
   "source": [
    "### Model architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cf5576",
   "metadata": {},
   "source": [
    "This architecture is a deep neural network designed for time series data. It uses a series of one-dimensional convolutional layers to extract relevant features from the input data, each with different filter sizes and numbers of filters. The filters are essentially small weight matrices that slide over the input data, looking for patterns that are useful for the task at hand.\n",
    "\n",
    "The kernel size parameter determines the width of the filter (i.e., how many time steps to look at together), while the number of filters determines how many different patterns the layer can learn. ReLU activation function is used in each convolutional layer, which is a simple non-linear function that introduces non-linearity into the model.\n",
    "\n",
    "Dropout is a regularization technique that helps prevent overfitting by randomly dropping out some of the neurons during training. This is done to reduce the co-adaptation of neurons and force the network to learn more robust features.\n",
    "\n",
    "The Flatten layer is used to convert the output of the convolutional layers into a 1-dimensional array that can be passed through a fully connected layer. A fully connected layer (Dense) with 1024 neurons and ReLU activation is then added, followed by another dropout layer to further prevent overfitting.\n",
    "\n",
    "Finally, a dense layer with just one neuron is added, which produces the output of the model. This neuron is not activated (i.e., no activation function is applied), because we want the output to be a continuous value."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
